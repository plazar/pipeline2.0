\documentstyle[12pt]{article}

\begin{document}

\title{PALFA Pipeline2.0 Documentation}
\author{Patrick Lazarus, Alex Samoilov, ... \\
        plazar@physics.mcgill.ca, alex@sequencefactory.com}
\date{\today}

\maketitle

\begin{abstract}
\end{abstract}

\section{Installation}
\subsection{Dependencies}
The PALFA pipeline has various dependencies. All of the following packages are required to run the pipeline.
\begin{itemize}
    \item PRESTO (https://github.com/scottransom/presto)
    \item psrfits\_utils (be sure to pull Kevin Stovall's version of psrfits\_utils. It contains the merging code required to analyse PALFA Mock spectrometer data. https://github.com/kstovall/psrfits\_utils.git)
    \item numpy (http://numpy.scipy.org/)
    \item M2Crypto (http://pypi.python.org/pypi/M2Crypto)
    \item PBSQuery (http://subtrac.sara.nl/oss/pbs\_python/wiki/TorqueInstallation)
    \item pyfits (http://www.stsci.edu/resources/software\_hardware/pyfits)
    \item pyodbc (http://pypi.python.org/pypi/pyodbc/)
    \item suds (http://pypi.python.org/pypi/suds)
\end{itemize}

\textit{NOTE: Some of these dependencies have requirements of their own. For example, PRESTO requires cfitsio, TEMPO, numpy, scipy, etc. Be sure to follow the installation instructions for each packages.}

\subsection{Getting Started}
Here we will present basic step-by-step instructions for setting up the pipeline.

\begin{description}
    \item[Step 1] Create a directory where you want the pipeline to be installed.

    \item[Step 2] Download the pipeline source files. This should be done by cloning the git repository on github.com: https://github.com/plazar/pipeline2.0.

        \smallskip

        \texttt{\$ git clone git://github.com/plazar/pipeline2.0.git}

        \smallskip
        
        \textit{NOTE: This will create a sub-directory called ``pipeline2.0".} 

    \item[Step 3] Add the pipeline directory to your PYTHONPATH environment variable. 

    \item[Step 4] Create configuration files using the examples provided. Modify the settings so they are appropriate for your system. Run the \texttt{sanity\_check.py} script to ensure all your configurations have the correct types, the neccessary directories exist and you have the correct privileges for files that must be read/written. 

    \item[Step 5] Set up the database. This is done using the \texttt{create\_database.py} script. 

    \item[Step 6] Start the downloader using \texttt{StartDownloader.py}.
        
        \smallskip
        
        \textit{NOTE: The downloader does not need to be run on the same computer as the job pooler. However, the directory where the downloader saves files must be accessible from the computer where the job pooler is being run.}
        
        \smallskip
        
        Alternatively, it is possible to add files to the job-tracker database without using the downloader. To do this use \texttt{add\_files.py}. Be sure to set the \texttt{delete\_rawdata} configuration to \texttt{False} if you do not want raw data files to be deleted when the pipeline no longer requires them. 

    \item[Step 7] Start the job pooler using \texttt{StartJobPool.py}. The job pooler will start submitting jobs when data files are finished downloading. 

    \item[Step 8] Start the uploader using \texttt{StartJobUploader.py}. The uploader will upload results to the common database when jobs are successfully processed.
\end{description}


\section{The Pipeline}
An overview of the pipeline.

\subsection{Features}
\begin{itemize}
    \item Sanity check of configurations
    \item Dynamic zapping
    \item Automatic download of data files
    \item File size checks when downloading
    \item Error emails
    \item Automatic (configurable) retry of failed jobs
    \item Check of results before uploading
    \item Automatic (configurable) deletion of data files (upon success or terminal failure)
\end{itemize}


\subsection{Components}
\subsubsection{Job-tracking Database}
Each job run through the pipeline passes through multiple stages before it is completed. To track the state of each job, and log its history a SQLite3 database is used. The database is simply a (readable/writable) binary file formatted according to the SQLite3 format. The database file can be access using SQL statements using python's sqlite3 module, which is part of python's standard library, as of python version 2.5.

Relevant links:
\begin{itemize}
    \item http://docs.python.org/library/sqlite3.html
    \item http://www.sqlite.org/
\end{itemize}

The job-tracker database governs all aspects of the pipeline. It maintains lists of requests for data, downloads, jobs, files associated with each job, processing attempts, and upload attempts.

Details for each table in the database are presented in Section \ref{sec:dbdetails}.

\subsubsection{Downloader}
\subsubsection{Job Pooler}
\subsubsection{Uploader}
\paragraph{Headers}

\paragraph{Candidates}

\paragraph{Diagnostics}
Diagnostics stored in the common database are split into two types: Numeric values and binary data. Some of the ``binary data" diagnostics are actually plain text files.

The following per-beam diagnostics are uploaded to the common database:
\begin{itemize}
    \item \textit{RFI mask percentage} (Numeric value) \hfill \\
        Percentage of data masked due to RFI.

    \item \textit{Num cands folded} (Numeric Value) \hfill \\
        The number of candidates folded.

    \item \textit{Num cands produced} (Numeric Value) \hfill \\
        The total number of candidates produced, including those with sigma lower than the folding threshold.

    \item \textit{Min sigma folded} (Numeric value) \hfill \\
        The smallest sigma value of all folded candidates from this beam.

    \item \textit{Num cands above threshold} (Numeric value) \hfill \\
        The number of candidates produced (but not necessarily folded) that are above the desired sigma threshold.
    
    \item \textit{RFIfind png} (Binary data) \hfill \\
        Output image produced by rfifind in png format.

    \item \textit{Accelcands list} (Binary data) \hfill \\
        The combined and sifted list of candidates produced by accelsearch.
    
    \item \textit{Zaplist used} (Binary data) \hfill \\
        The list of frequencies and ranges zapped from the power spectrum before searching this beam.
\end{itemize}

\subsection{Objects Tracked}
\paragraph{Restores}

\paragraph{Downloads}

\paragraph{Jobs}

\paragraph{Uploads}


\section{Reference}
\subsection{Executables}
\paragraph{\texttt{StartDownloader.py}}
A script to start the downloader background process.

usage: \texttt{StartDownloader.py}

\paragraph{\texttt{StartJobPool.py}}
A script to start the job pooler background process.

usage: \texttt{StartJobPool.py}

\paragraph{\texttt{StartJobUploader.py}}
A script to start the results uploader background process.

usage: \texttt{StartJobUploader.py}

\subsection{Generic Queue Manager Interface}
The Pipeline interacts with the system's queue manager using an abstract interface. This generic interface can be implemented with the specific queue manager installed on the system. The system-specific queue manager class must be derived from the \texttt{PipelineQueueManager} class. Each of the following methods of \texttt{PipelineQueueManager} must be implemented in the derived class:

\begin{description}
    \item[submit] Submits a job to queue manager with files list and output directory. Must return a unique string identifier for the submitted job. Raises ValueError if job was not submitted by queue manager.

    \item[is_running] Given a unique identifier for a job, must return True or False whether the job is running or not, respectively.

    \item[is_processing_file] Given a string path of a file, return True or False whether or not the job processing the input filename is running.

    \item[delete] 


\subsection{Configurations}

\subsection{Database Details}
\label{sec:dbdetails}

\end{document}
